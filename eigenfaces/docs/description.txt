Assumptions:
	- training images are grayscale
	- training images have the same dimensions
	- training images are in pgm format
	- eye and mouth positions are around the same in all the training and test images (this is somewhat optional but has an effect on the predicting accuracy)
	- input files of training and test sets are in the correct format (e.g. all specified path exists)

Few wrods about PCA (and the answer to the input file questions):
	On the highest level PCA has only a few steps. Firstly it creates "principal component" images out of which you can mix out the original training images. (E.g. for training image no. 7 you use 0.5 part of principal component image no. 1, 0.3 part of no.2, and 0.2 of no. 3.) These principal component images are sorted in a way that the earlier principal component images describe more of the original/training images than the latter ones. This means that the last principal component images are not that important. To speed up recognition we can drop the unimportant principal component images in two ways; either we explicitly specify how many principal component images we want to use (there are exactly that many principal component images as there are training images) or we specify how much of explanatory power they should have (i.e. how big a part of the original images can be mixed out of them). After this we calculate how we can mix out the original images from the principal component images. The principal component images and the weights could be saved in a database at this point so that they can be reused for the recognition (I did not implement such a database). Based on the training image weights (and the principal component images) we can calculate for any new (test) images which image has the closest weights to its representation (and ultimately which image is the most likely to be about the same thing).
	
Implementation:
	Files:
		data_struct		-	representation of image/label data
		io				-	input/output operations (reading datasets and images)
		calculations	-	lower level parts of the PCA algorithm
		main			- 	handling user input and higher level parts of the PCA algorithm
		
	Design decisions:
		- Most PCA descriptions use the images as columns of a matrix. As some other implementations I used the rows (because it is more efficient to append rows toa  cv::Mat tha a column). Using this representation means that (compared to most descriptions) the transpose of the image matrix is used.
		- Instead of using SVD on the <number of images> * <size of an image> matrix, I used the trick explained at https://en.wikipedia.org/wiki/Eigenface#Computing_the_eigenvectors  This achieves the same thing but has to decompose only a <number of images> * <number of images> matrix which is probably a lot smaller (and decomposition is expensive).
		- I used Eigen decomposition instead of the popular SVD choice. Since I didn't use the SVD trick (also explained on Wikipedia and would contradict the previous trick) it doesn't really matter. They achieve the exact same results and they use the same magnitude of steps so there is no good answer here. If you'd like to I can change it of course but there is no upside to it.
		- Other then these you probably can follow the high level ideas in the main function. It is pretty close to the level they are explained in most descriptions.
		
	Useful to know:
		- During reading the input there is a "pgmToMatRow" function. If you would like to read in another type of image then you would have to check the range in which the pixel values can be. (For the pgm a quick google search tells us that it is from the [0,255] range) and you should change the "intensity_range" variable to this range (255-0) to make it work with that image type.
		- For different types of images this resource https://stackoverflow.com/questions/13577164/pixel-type-of-imread can be useful too. To read in a 16 bit/pixel tiff image you just have to add ', cv::IMREAD_GRAYSCALE | cv::IMREAD_ANYDEPTH' to the parameters of the imread function (in the 'readPgm' function).
		- If you call the executable without parameters, it prints out a short help
		- In the 'input' folder you can find an example training input file ('training.txt') using only a part of the images (with correct labels). There is the 'full_data.txt' containing all the images with the correct labels and there is the 'test.txt' which is an example for test input and contains only images that are not present in the 'training.txt'. (There is not much sense in classifying the same images we used to train our model with.)
		
	Format of TRAINING input file:
		- the input file should contain one line for each training image in the following format:
			<relative path to the image from the executable (!) OR absolute path to the image><tab><a nonnegative whole number, the label of the image (person's #ID)>
	
	Format of TEST input file:
		- the input file should contain one line for each test image in the following format:
			<relative path to the image from the executable (!) OR absolute path to the image>